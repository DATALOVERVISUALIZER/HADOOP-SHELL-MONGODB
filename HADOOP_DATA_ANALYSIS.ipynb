{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Introduction to Hadoop Ecosystem\n",
    "\n",
    "**by Leyla YiÄŸit**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Start SSH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stopping OpenBSD Secure Shell server: sshd.\n",
      "Starting OpenBSD Secure Shell server: sshd.\n"
     ]
    }
   ],
   "source": [
    "sudo service ssh stop\n",
    "sudo service ssh start"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And check that sshd works:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "sshd is running.\n"
     ]
    }
   ],
   "source": [
    "service ssh status"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Configuration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "/opt/hadoop-2.9.2\n",
      "/etc/hadoop\n",
      "/opt/hadoop-2.9.2\n",
      "/opt/sqoop/bin:/opt/hadoop-2.9.2/bin:/opt/hadoop-2.9.2/sbin:/opt/hadoop-2.9.2/bin/:/usr/local/sbin:/usr/local/bin:/usr/sbin:/usr/bin:/sbin:/bin:/usr/lib/postgresql/11/bin\n"
     ]
    }
   ],
   "source": [
    "echo $HADOOP_HOME\n",
    "echo $HADOOP_CONF_DIR\n",
    "echo $HADOOP_PREFIX\n",
    "echo $PATH"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "capacity-scheduler.xml      httpfs-env.sh            mapred-env.sh\n",
      "configuration.xsl           httpfs-log4j.properties  mapred-queues.xml.template\n",
      "container-executor.cfg      httpfs-signature.secret  mapred-site.xml\n",
      "core-site.xml               httpfs-site.xml          mapred-site.xml.template\n",
      "hadoop-env.cmd              kms-acls.xml             slaves\n",
      "hadoop-env.sh               kms-env.sh               ssl-client.xml.example\n",
      "hadoop-metrics.properties   kms-log4j.properties     ssl-server.xml.example\n",
      "hadoop-metrics2.properties  kms-site.xml             yarn-env.cmd\n",
      "hadoop-policy.xml           log4j.properties         yarn-env.sh\n",
      "hdfs-site.xml               mapred-env.cmd           yarn-site.xml\n"
     ]
    }
   ],
   "source": [
    "ls $HADOOP_CONF_DIR"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
      "<!--\n",
      "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "  you may not use this file except in compliance with the License.\n",
      "  You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "  Unless required by applicable law or agreed to in writing, software\n",
      "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "  See the License for the specific language governing permissions and\n",
      "  limitations under the License. See accompanying LICENSE file.\n",
      "-->\n",
      "\n",
      "<!-- Put site-specific property overrides in this file. -->\n",
      "\n",
      "<configuration>\n",
      "    <property>\n",
      "        <name>fs.defaultFS</name>\n",
      "        <value>hdfs://localhost</value>\n",
      "    </property>\n",
      "    <property>\n",
      "      <name>hadoop.tmp.dir</name>\n",
      "      <value>/hadoop-data</value>\n",
      "    </property>\n",
      "        <property>\n",
      "        <name>hadoop.security.authentication</name>\n",
      "        <value>simple</value>\n",
      "    </property>\n",
      "    <property>\n",
      "        <name>hadoop.security.authorization</name>\n",
      "        <value>false</value>\n",
      "    </property>\n",
      "</configuration>\n"
     ]
    }
   ],
   "source": [
    "cat $HADOOP_CONF_DIR/core-site.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\" encoding=\"UTF-8\"?>\n",
      "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
      "<!--\n",
      "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "  you may not use this file except in compliance with the License.\n",
      "  You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "  Unless required by applicable law or agreed to in writing, software\n",
      "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "  See the License for the specific language governing permissions and\n",
      "  limitations under the License. See accompanying LICENSE file.\n",
      "-->\n",
      "\n",
      "<!-- Put site-specific property overrides in this file. -->\n",
      "\n",
      "<configuration>\n",
      "    <property>\n",
      "        <name>dfs.replication</name>\n",
      "        <value>1</value>\n",
      "    </property>\n",
      "     <property>\n",
      "         <name>dfs.data.dir</name>\n",
      "         <value>/hadoop-data/dfs/data</value>\n",
      "         <final>true</final>\n",
      "     </property>\n",
      "     <property>\n",
      "         <name>dfs.name.dir</name>\n",
      "         <value>/hadoop-data/dfs/name</value>\n",
      "         <final>true</final>\n",
      "     </property>\n",
      "</configuration>\n"
     ]
    }
   ],
   "source": [
    "cat $HADOOP_CONF_DIR/hdfs-site.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\"?>\n",
      "<?xml-stylesheet type=\"text/xsl\" href=\"configuration.xsl\"?>\n",
      "<!--\n",
      "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "  you may not use this file except in compliance with the License.\n",
      "  You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "  Unless required by applicable law or agreed to in writing, software\n",
      "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "  See the License for the specific language governing permissions and\n",
      "  limitations under the License. See accompanying LICENSE file.\n",
      "-->\n",
      "\n",
      "<!-- Put site-specific property overrides in this file. -->\n",
      "\n",
      "<configuration>\n",
      "    <property>\n",
      "        <name>mapreduce.framework.name</name>\n",
      "        <value>yarn</value>\n",
      "    </property>\n",
      "    <property>\n",
      "      <name>mapred.job.tracker</name>\n",
      "      <value>localhost</value>\n",
      "      <description>The host and port that the MapReduce job tracker runs at.  I$  \n",
      "      </description>\n",
      "    </property>\n",
      "</configuration>\n"
     ]
    }
   ],
   "source": [
    "cat $HADOOP_CONF_DIR/mapred-site.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<?xml version=\"1.0\"?>\n",
      "<!--\n",
      "  Licensed under the Apache License, Version 2.0 (the \"License\");\n",
      "  you may not use this file except in compliance with the License.\n",
      "  You may obtain a copy of the License at\n",
      "\n",
      "    http://www.apache.org/licenses/LICENSE-2.0\n",
      "\n",
      "  Unless required by applicable law or agreed to in writing, software\n",
      "  distributed under the License is distributed on an \"AS IS\" BASIS,\n",
      "  WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
      "  See the License for the specific language governing permissions and\n",
      "  limitations under the License. See accompanying LICENSE file.\n",
      "-->\n",
      "<configuration>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.aux-services</name>\n",
      "    <value>mapreduce_shuffle</value>\n",
      "  </property>\n",
      "  <property>\n",
      "    <name>yarn.nodemanager.aux-services.mapreduce_shuffle.class</name>\n",
      "    <value>org.apache.hadoop.mapred.ShuffleHandler</value>\n",
      "  </property>\n",
      "</configuration>\n"
     ]
    }
   ],
   "source": [
    "cat $HADOOP_CONF_DIR/yarn-site.xml"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "localhost\n"
     ]
    }
   ],
   "source": [
    "cat $HADOOP_CONF_DIR/slaves"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Starting namenodes on [localhost]\n",
      "localhost: starting namenode, logging to /opt/hadoop-2.9.2/logs/hadoop-hadoop-namenode-jupyter-serhatcevikel-2dbdm-5f2018-5ftest2-2de4l1fnac.out\n",
      "localhost: starting datanode, logging to /opt/hadoop-2.9.2/logs/hadoop-hadoop-datanode-jupyter-serhatcevikel-2dbdm-5f2018-5ftest2-2de4l1fnac.out\n",
      "Starting secondary namenodes [0.0.0.0]\n",
      "0.0.0.0: starting secondarynamenode, logging to /opt/hadoop-2.9.2/logs/hadoop-hadoop-secondarynamenode-jupyter-serhatcevikel-2dbdm-5f2018-5ftest2-2de4l1fnac.out\n"
     ]
    }
   ],
   "source": [
    "start-dfs.sh 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "starting yarn daemons\n",
      "starting resourcemanager, logging to /opt/hadoop-2.9.2/logs/yarn-root-resourcemanager-jupyter-serhatcevikel-2dbdm-5f2018-5ftest2-2de4l1fnac.out\n",
      "localhost: starting nodemanager, logging to /opt/hadoop-2.9.2/logs/yarn-hadoop-nodemanager-jupyter-serhatcevikel-2dbdm-5f2018-5ftest2-2de4l1fnac.out\n"
     ]
    }
   ],
   "source": [
    "start-yarn.sh"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "273 DataNode\n",
      "177 NameNode\n",
      "756 Jps\n",
      "725 NodeManager\n",
      "630 ResourceManager\n",
      "445 SecondaryNameNode\n"
     ]
    }
   ],
   "source": [
    "jps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Safe mode is ON\n",
      "Configured Capacity: 1052500430848 (980.22 GB)\n",
      "Present Capacity: 545214529536 (507.77 GB)\n",
      "DFS Remaining: 542460252160 (505.21 GB)\n",
      "DFS Used: 2754277376 (2.57 GB)\n",
      "DFS Used%: 0.51%\n",
      "Under replicated blocks: 0\n",
      "Blocks with corrupt replicas: 0\n",
      "Missing blocks: 0\n",
      "Missing blocks (with replication factor 1): 0\n",
      "Pending deletion blocks: 0\n",
      "\n",
      "-------------------------------------------------\n",
      "Live datanodes (1):\n",
      "\n",
      "Name: 127.0.0.1:50010 (localhost)\n",
      "Hostname: jupyter-serhatcevikel-2dbdm-5f2018-5ftest2-2de4l1fnac\n",
      "Decommission Status : Normal\n",
      "Configured Capacity: 1052500430848 (980.22 GB)\n",
      "DFS Used: 2754277376 (2.57 GB)\n",
      "Non DFS Used: 507269124096 (472.43 GB)\n",
      "DFS Remaining: 542460252160 (505.21 GB)\n",
      "DFS Used%: 0.26%\n",
      "DFS Remaining%: 51.54%\n",
      "Configured Cache Capacity: 0 (0 B)\n",
      "Cache Used: 0 (0 B)\n",
      "Cache Remaining: 0 (0 B)\n",
      "Cache Used%: 100.00%\n",
      "Cache Remaining%: 0.00%\n",
      "Xceivers: 1\n",
      "Last contact: Tue Jan 22 22:52:34 UTC 2019\n",
      "Last Block Report: Tue Jan 22 22:52:11 UTC 2019\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "hdfs dfsadmin -report 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "here are the output files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "hdfs dfs -mkdir -p /output 2>&1 | grep -Pv \"^WARNING\"\n",
    "hdfs dfs -mkdir -p /output/imdbemre 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "hdfs dfs -rm -r -f /output/imdbemre/* 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ls ~/data/imdb/tsv/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "mkdir -p ~/data/split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "#take first 50k rows of title basics, split to 4. \n",
    "cd ~/data/split && \\\n",
    "head -40000 ~/data/imdb/tsv/title.basics.tsv | tail -n+2 | split -l 10000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "xaa  xab  xac  xad\n"
     ]
    }
   ],
   "source": [
    "ls ~/data/split"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "FILE CONTENT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tt0000001\tshort\tCarmencita\tCarmencita\t0\t1894\t\\N\t1\tDocumentary,Short\n",
      "tt0000002\tshort\tLe clown et ses chiens\tLe clown et ses chiens\t0\t1892\t\\N\t5\tAnimation,Short\n",
      "tt0000003\tshort\tPauvre Pierrot\tPauvre Pierrot\t0\t1892\t\\N\t4\tAnimation,Comedy,Romance\n",
      "tt0000004\tshort\tUn bon bock\tUn bon bock\t0\t1892\t\\N\t\\N\tAnimation,Short\n",
      "tt0000005\tshort\tBlacksmith Scene\tBlacksmith Scene\t0\t1893\t\\N\t1\tShort\n",
      "tt0000006\tshort\tChinese Opium Den\tChinese Opium Den\t0\t1894\t\\N\t1\tShort\n",
      "tt0000007\tshort\tCorbett and Courtney Before the Kinetograph\tCorbett and Courtney Before the Kinetograph\t0\t1894\t\\N\t1\tShort,Sport\n",
      "tt0000008\tshort\tEdison Kinetoscopic Record of a Sneeze\tEdison Kinetoscopic Record of a Sneeze\t0\t1894\t\\N\t1\tDocumentary,Short\n",
      "tt0000009\tmovie\tMiss Jerry\tMiss Jerry\t0\t1894\t\\N\t45\tRomance\n",
      "tt0000010\tshort\tEmployees Leaving the LumiÃ¨re Factory\tLa sortie de l'usine LumiÃ¨re Ã  Lyon\t0\t1895\t\\N\t1\tDocumentary,Short\n"
     ]
    }
   ],
   "source": [
    "head ~/data/split/xaa"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# only mapper"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1894\n",
      "1892\n",
      "1892\n",
      "cut: write error: Broken pipe\n",
      "\n",
      "1919\n",
      "1919\n",
      "1919\n",
      "cut: write error: Broken pipe\n",
      "\n",
      "1929\n",
      "1929\n",
      "1930\n",
      "cut: write error: Broken pipe\n",
      "\n",
      "1938\n",
      "1938\n",
      "1938\n",
      "cut: write error: Broken pipe\n",
      "\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for file in ~/data/split/*; do cut -f 6 $file | \\\n",
    "head -3; echo; done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mapper + combiner"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: column count is one - did you provide the correct delimiter?\n",
      "1892 3\n",
      "1893 1\n",
      "1894 6\n",
      "Warning: column count is one - did you provide the correct delimiter?\n",
      "1912 1\n",
      "1915 4\n",
      "1916 5\n",
      "Warning: column count is one - did you provide the correct delimiter?\n",
      "1919 1\n",
      "1926 1\n",
      "1927 1\n",
      "Warning: column count is one - did you provide the correct delimiter?\n",
      "1933 2\n",
      "1934 2\n",
      "1935 6\n"
     ]
    }
   ],
   "source": [
    "\n",
    "for file in ~/data/split/*; do cut -f 6 $file | \\\n",
    "q \"select c1, count(c1) from - group by c1\" | \\\n",
    "head -3; done\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Warning: column count is one - did you provide the correct delimiter?\n",
      "Warning: column count is one - did you provide the correct delimiter?\n",
      "Warning: column count is one - did you provide the correct delimiter?\n",
      "Warning: column count is one - did you provide the correct delimiter?\n",
      "\n",
      "\n",
      "2019-01-22 22:59                                                  Page 1\n",
      "\n",
      "\n",
      "1892 3\t\t\t1916 1235\t\t1940 1116\n",
      "1893 1\t\t\t1917 1211\t\t1941 1055\n",
      "1894 6\t\t\t1918 1038\t\t1942 1148\n",
      "1895 18\t\t\t1919 1076\t\t1943 981\n",
      "1896 105\t\t1920 941\t\t1944 888\n",
      "1897 38\t\t\t1921 953\t\t1945 791\n",
      "1898 44\t\t\t1922 892\t\t1946 834\n",
      "1899 46\t\t\t1923 831\t\t1947 838\n",
      "1900 82\t\t\t1924 888\t\t1948 671\n",
      "1901 35\t\t\t1925 1021\t\t1949 77\n",
      "1902 35\t\t\t1926 997\t\t1950 20\n",
      "1903 57\t\t\t1927 1009\t\t1951 4\n",
      "1904 21\t\t\t1928 1007\t\t1952 4\n",
      "1905 33\t\t\t1929 965\t\t1953 3\n",
      "1906 41\t\t\t1930 937\t\t1954 5\n",
      "1907 49\t\t\t1931 1023\t\t1957 3\n",
      "1908 158\t\t1932 1072\t\t1959 1\n",
      "1909 307\t\t1933 1064\t\t1961 1\n",
      "1910 363\t\t1934 1178\t\t1965 1\n",
      "1911 506\t\t1935 1188\t\t1970 1\n",
      "1912 601\t\t1936 1271\t\t1973 1\n",
      "1913 980\t\t1937 1269\t\t1983 1\n",
      "1914 1225\t\t1938 1150\t\t1993 1\n",
      "1915 1475\t\t1939 1108\t\t2001 1\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# mapper + combiner + reducer\n",
    "for file in ~/data/split/*; do cut -f 6 $file | \\\n",
    "q \"select c1, count(c1) from - group by c1\"; done | \\\n",
    "q \"select c1, sum(c2) from - group by c1\" | \\\n",
    "pr --columns=3\n"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "#HDFS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "ename": "",
     "evalue": "1",
     "output_type": "error",
     "traceback": []
    }
   ],
   "source": [
    "hdfs dfs -put -f ~/data/split /data 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Found 4 items\n",
      "-rw-r--r--   1 hadoop supergroup     800158 2019-01-22 23:01 /data/split/xaa\n",
      "-rw-r--r--   1 hadoop supergroup     740302 2019-01-22 23:01 /data/split/xab\n",
      "-rw-r--r--   1 hadoop supergroup     770142 2019-01-22 23:01 /data/split/xac\n",
      "-rw-r--r--   1 hadoop supergroup     792916 2019-01-22 23:01 /data/split/xad\n"
     ]
    }
   ],
   "source": [
    "hdfs dfs -ls /data/split 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "Displays sizes of files and directories contained in the given directory or the length of a file in case its just a file.\n",
    "\n",
    "Options:\n",
    "\n",
    "The -s option will result in an aggregate summary of file lengths being displayed, rather than the individual files.\n",
    "The -h option will format file sizes in a \"human-readable\" ,example like below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "du: `/home/hadoop/data/split': No such file or directory\n",
      "177017193   /data/comtrade_s1\n",
      "126864458   /data/he_sisli\n",
      "1399713628  /data/imdb\n",
      "550766009   /data/ncdc\n",
      "471327764   /data/ngrams\n",
      "3103518     /data/split\n"
     ]
    }
   ],
   "source": [
    "hdfs dfs -du  ~/data/split /data 2>&1 | grep -Pv \"^WARNING\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "stat: `/home/hadoop/data/he_sisli': No such file or directory\n"
     ]
    }
   ],
   "source": [
    "hdfs dfs -stat /data/he_sisli 2>&1 | grep -Pv \"^WARNING\""
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Bash",
   "language": "bash",
   "name": "bash"
  },
  "language_info": {
   "codemirror_mode": "shell",
   "file_extension": ".sh",
   "mimetype": "text/x-sh",
   "name": "bash"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": false,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {},
   "toc_section_display": true,
   "toc_window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
